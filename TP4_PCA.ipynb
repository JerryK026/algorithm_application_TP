{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. PCA가 왜 중요한가?\n",
    "\n",
    "pca는 다른 알고리즘에 영향을 많이 주었고, 유연성이 뛰어나기 때문에 그 활용성이 크다. 특히, PCA는 패턴 인식 분야에서 자주 사용되는 기본 방법 중의 하나이다. pca는 정보 중복성을 최소화한 것이기 때문에, 데이터를 축소한 것이라고 볼 수도 있다. 따라서 dimension reduction을 진행하면서 용량은 줄이고 효율성을 늘릴 수 있는 장점도 지닌다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. PCA는 일상생활에 어디에 적용되는가\n",
    "\n",
    "주성분 분석(PCA)은 사실 선형대수학이라기 보다는 선형대수학의 활용적인 측면이 강하며 영상인식, 통계 데이터 분석(주성분 찾기), 데이터 압축(차원감소), 노이즈 제거 등 다양한 활용을 갖는다.\n",
    "\n",
    "출처 : https://darkpgmr.tistory.com/110"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. PCA를 활용 가능한 주요 알고리즘이나 패키지에는 무엇이 있는가?\n",
    "\n",
    "pca를 활용하는 패키지는 대표적으로 sklearn의 pca가 있다. sklearn의 pca는 componets를 정해주고 데이터를 fit하여 사용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99244289 0.00755711]\n",
      "[6.30061232 0.54980396]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "PCA(n_components=2)\n",
    "print(pca.explained_variance_ratio_)\n",
    "# [0.9924... 0.00755...]\n",
    "print(pca.singular_values_)\n",
    "# [6.30061... 0.54980...]\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. PCA의 핵심 아이디어는 무엇인가?\n",
    "\n",
    "\n",
    "pca는 일종의 data dimension reduction의 일종이다. dimenstion reduction이란, 여러 차원의 데이터를 저차원으로 바꾸는 것이다. dimension reduction에는 크게 2종류로 나뉘는데, 데이터 중 사용할 것만 가져오는 feature selection(데이터의 가공 x)와 전체의 데이터를 가공해서 사용하는 feature projection이 있다. PCA는 feature projection의 대표적인 알고리즘이다.\n",
    "\n",
    "PCA는 기존 데이터를 선형적으로 맵핑해서 차원 축소를 한다. 여기서 선형적으로 쓴다는 것은 곧 covariance를 활용한다는 말과 마찬가지이다. PCA의 특징을 정리하자면 3가지로 나눌 수 있다.\n",
    "\n",
    "특징 1 : 선형적으로 data를 projection한다. Linear transformation이라는 것은 결국 회전하고 늘린다는 것이다.라는 것을 유의한다.\n",
    "![pca_linear_transform_of_data](./images/pca_linear_transform_of_data.png)\n",
    "\n",
    "\n",
    "특징 2 : 축을 변경한다. 우리가 차원을 축소하는 이유는 데이터를 더 잘 설명하기 위해서이다. 여기서 차원을 축소한다는 것은 결국 축을 변경한다는 말과 같기도 하다. 따라서, 데이터들을 더 설명하기 쉬운 축을 찾아내는 것에 그 목표가 있다.\n",
    "![pca_cov_matrix_of_x](./images/pca_cov_matrix_of_x.png)\n",
    "\n",
    "\n",
    "특징 3 : 중복성을 최소화한다. 중복성을 가진다는 것은 어떻게 보면 무의미하게 데이터를 너무 많이 가지고 있을 수도 있다는 뜻이다. 따라서 covariance를 파악해서, 상관관계가 조금이라도 있지 않다면, 그것은 중복성이 없기 때문에 최소화가 잘 되도록 재구성된 dataset이라고 볼 수 있다.\n",
    "\n",
    "정리하자면, Y = PX에서 수직 matrix인 P를 구하는 것이 목표이다. 여기서 P를 구하는 과정은 1.X의 covariance Cx를 구하고, 2. Cx의 ev(eigen vector)를 구해서 3.eigen vector의 eigen value를 큰 값 별로 내림차순해 정리한다. 그것이 바로 P이다.\n",
    "![pca_solution](./images/pca_solution.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
