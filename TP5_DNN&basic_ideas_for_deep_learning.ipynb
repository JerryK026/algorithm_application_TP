{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. DNN이 왜 중요한가?\n",
    "\n",
    "연속형, 범주형 변수에 상관없이 모두 분석 가능하고, 입력 변수들 간의 비선형 조합이 가능하다. 이는 다른 신경망에 비해 DNN이 가지는 가장 좋은 장점 중 하나이다. 또, 예측력이 다른 머신러닝 기법들에 비해 상대적으로 좋은 경우가 많고, feature extraction이 자동으로 수행되기 때문에 변수 선택의 비용을 줄여준다. data가 많을수록 성능이 계속 좋아지는 것도 장점이다.\n",
    "\n",
    "출처 : https://m.blog.naver.com/tjdudwo93/221072421443"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. DNN은 일상생활 어디에 활용되는가?\n",
    "\n",
    "여러 고차원에서 데이터 분류가 가능해지면서 다양한 용도로 사용된다. 영상처리, 음성인식, 자연어 처리 등에 사용되며, 많은 알고리즘에 영향을 끼쳤다. 예를 들면 CNN, RNN, LSTM, GRU ...가 있다.\n",
    "\n",
    "출처 : http://physics2.mju.ac.kr/juhapruwp/?p=1517"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. DNN을 활용가능한 알고리즘이나 패키지에는 무엇이 있는가?\n",
    "\n",
    "DNN을 활용한 알고리즘에는 CNN, RNN, LSTM, GRU등이 있다. hidden layer를 여러개 쌓는 알고리즘들에 영향을 끼쳤다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. DNN의 핵심 아이디어는 무엇인가?\n",
    "\n",
    "representation learning : 우리가 사물을 인지할 때는 경험을 통해서 아는 것이 아니라, 사물을 해당 사물이라 인지할 수 있는 무언가가 우리 뇌 안에 있을 것이다. -> 내가 바라보는 사물을 어떻게 표현할 수 있을지 혹은 그 핵심에는 무엇이 있을지를 기계에 넣어주려면 어떻게 해야할까? -> 만약 우리가 무언가에 잘 알고 있다면, 완성된 데이터를 지웠다가 완벽히 복원할 수 있을 것이다. 라는 아이디어에서 고안되었다. 즉, feature를 종합해, representation(사물을 설명할 수 있는 핵심 정보)를 찾는 것이다.\n",
    "\n",
    "과거에는 one-hot 방식(기호 방식)으로, vector에서 해당 데이터의 특징은 무시하고, class에 들어 있는 값만으로 데이터를 판단했다. 하지만, 현재는 점점 distributed representation으로, 해당하는 데이터에 모든 관련 데이터들을 저차원에 실수로 뿌려서 이를 토대로 데이터를 판단한다. 이러한 변화를 paradigm shift라 한다.\n",
    "\n",
    "인지과학 > 인공지능 > 머신러닝 > 딥러닝 | 딥러닝과 데이터 마이닝은 겹치는 기술이 많다.\n",
    "![dnn_position_of_deep_learning](./images/dnn_position_of_deep_learning.png)\n",
    "\n",
    "hidden variable이란, 가상의 값인데 무엇이든 될 수 있다. 즉, 추측만이 가능한 값인데 이 hidden variable h의 값에, 실측 가능한 변수 x, y, z, ...를 연관시켜서 일정한 구조를 만들어서 의미있는 무언가로 만든다. 이 hidden variable이 곧 representation learning에서 언급한 사물을 표현한 무언가이다. 이러한 구조를 이루기 위해서는 많은 데이터를 필요로 한다. hidden layer는 이러한 hidden variable이 모인 것이다. 이 hidden layer도 여러층으로 쌓을 수 있는데, 이 층은 쌓을 수록 표현능력이 좋아지지만, h의 수가 많아지고, 결국 input data를 훨씬 더 필요로 한다. hidden layer는 위로 올라갈 수록 input data들을 추상화 하는 역할이라고 할 수 있다. 결국, output은 input data들의 궁극의 추상화 결과물이라 볼 수도 있다.\n",
    "![dnn_hidden_layers](./images/dnn_hidden_layers.png)\n",
    "\n",
    "Machine Learning 과정은 크게 5blocks로 나눌 수 있다. Data Preparation(file을 tensor구조로 바꾼다) -> model implementation -> loss implementation(object function) -> updater implementation(오차를 얼마나 줄일건지) -> iterative learning(학습을 몇번 할건지) 순이다. 이 중, model implemetation과정과 loss implementation 과정은 유형화가 가능하다. 즉, 누가 만들어 놓으면 가져다 쓸 수 있다는 것이다. 이 유형화로 인해 머신러닝의 진입장벽이 크게 낮아지고 있다.\n",
    "![DL_blocks](./images/DL_blocks.png)\n",
    "\n",
    "Tensor : 많은 data들은 각자의 형식이 있지만, 실수 형식인 tensor로 단일화 시켜준다(data interface). 즉, 중간과정이 형식이라고 할 수 있다.\n",
    "\n",
    "epoch size : 전체를 돌려 몇번 확인했는지이다.\n",
    "\n",
    "step : parameter를 update하는 횟수이다.\n",
    "\n",
    "batch gradient descent : 전체 데이터를 parameter로 넣어서 스캔해 원하는 모델 세타(θ)를 찾겠다는 뜻이다. 즉. 일괄적으로 계산한다.\n",
    "\n",
    "stochastic gradient descent : 한 번의 iteration에 randomly shuffle한 예제 1개씩만 사용한다. 이렇게 해서 찾아낸 update factor로 새로운 model을 만든다.\n",
    "\n",
    "mini-batch gradient descent : batch size만큼만 update에 활용한다. 한 번의 iteration 후, 다음 예제로 넘어간다. 일반적인 training에선 mini-batch gradient descent를 가정한다. BGD와 SGD의 중간 정도라고 생각하면 된다.\n",
    "\n",
    "loss는 batch size와 learning rate에 영향을 많이 받는다. learning rate는 현재의 parameter, 즉 기울기인데. 이것을 얼마나 바꿔줄 것인지에 따라 변화하는 값이 달라진다. 여기서 loss값은 성능이 아니다.\n",
    "![DL_loss](./images/DL_loss.png)\n",
    "\n",
    "overfitting : 훈련 데이터에만 너무 적응해서 실제의 데이터를 잘 적용하지 못하는 걸 뜻한다.\n",
    "![DL_accuracy](./images/DL_accuracy.png)\n",
    "\n",
    "data는 train data, validation data(monitoring용), test data로 이루어져 있다. train data를 학습시켜, validation data와 비교를 해 test한다. 이는 성능을 보겠다는 것이다. 그래서, 채점을 해 틀렸으면 얼마나 틀렸는지 비교해서 재학습한다. 이러한 학습을 반복한 후, test data와 비교해서 real world에 해당되는 domain에서는 어떤 성능을 보일지 확인한다.\n",
    "\n",
    "AI 문제를 풀 때는 대부분 classfication(교사방식supervised, data에 맞는 label을 부여해줘서 class를 구현) 혹은 clustering(비교사방식unsupervised, data 내에서 스스로 partitioning하도록 구현)으로 나누어서 해결하려 한다.\n",
    "![DL_supervised&classfication](./images/DL_supervised&classfication.png)\n",
    "\n",
    "classfication을 할 때는 class개수를 정의하고, category로 나누는 게 중요하다. category를 나눌 땐 기술 구현이 가능하고, 사용자가 쓸 수 있어야 한다. 또, socring할 때 데이터의 일관성 있게 tagging해야 한다. 즉, 기준을 잘 설정해야 한다. 만약, 기준을 잘 설정해주지 않는다면, 분별력이 사라질 것이다. 예측과 정답을 비교할 때는 서로 공평하도록 scale을 해주어야 한다. 이렇게 같은 축으로 만들어주는 걸 softmax라고 한다.\n",
    "![DL_softmax](./images/DL_softmax.png)\n",
    "\n",
    "예측과 정답을 비교할 땐, 두 점수들 간의 차이를 수치화해야 한다. class들을 다 더하면 1이니 결국 확률분포와 같다는 점을 인지하고, 차원 수가 1개일 땐, MSE를 사용한다. 아니라면 cross Entropy를 이용한다. regression이란 차이를 비교하는 방법이다.\n",
    "\n",
    "오류를 작아지게 하려면 어떻게 해야할까? 방향을 결정해야 한다. 극점을 찾아 기울기게 작아지는 방향으로 가게 해서 오류를 작게 할 수 있다.\n",
    "\n",
    "결국 neural network란 parameter를 순차적으로 update해가는 것임을 명심한다. 정리하자면, input data를 숫자 data로 만들기 위해 network를 설정해서 집어넣고 output을 만들어내는 과정이라고 할 수 있다.\n",
    "\n",
    "정리 :\n",
    "reference representation : 정답을 어떻게 표현할까? -> 대부분 one-hot으로, class 수가 n이라면 n차원으로\n",
    "\n",
    "socring normalization : scale을 맞춰주기 위해 softmax\n",
    "\n",
    "cost function design : cost가 0으로 가는 방향으로. -> 대부분 back propagation\n",
    "\n",
    "parameter update\n",
    "![DL_back_propagaiton](./images/DL_back_propagaiton.png)\n",
    "여기서 reference representation빼고 재활용이 가능하다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
