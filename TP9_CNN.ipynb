{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.CNN이 왜 중요한가?\n",
    "\n",
    "CNN은 특징을 직접 학습하기 때문에 feature를 사람이 직접 추출할 필요가 없다. 즉, 비용이 크게 감소하는데 비해 인식 결과 수준이 매우 높다. 기존 네트워크를 바탕으로 한 새로운 인식 작업을 위해 CNN을 재학습하여 사용하는 것이 가능하다. 따라서 효율성이 매우 크다. 또, 이미지에서 객체를 민감하게 감지하기 때문에 CNN은 객체 감지 분야에서 자주 사용된다.\n",
    "\n",
    "+기존의 신경망은 데이터의 형상이 무시되기 때문에 글자에 변형이 조금만 생기더라고 다른 글자로 인식하기 때문에 새로운 학습 데이터를 넣어주지 않으면 좋은 결과를 기대하기가 어렵다. 즉, 본질적인 패턴을 읽지 못하므로 인식을 위해 다양한 데이터를 필요로 한다. 그러나 CNN은 원본 이미지로 여러개의 feature map을 만들어 분류하는 완전 연결 계층으로, 이미지의 특징을 추출하기 때문에 이미지가 변형되더라도 잘 인식할 수 있다.\n",
    "\n",
    "출처 : https://kr.mathworks.com/solutions/deep-learning/convolutional-neural-network.html\n",
    "\n",
    "참고 : https://everyday-deeplearning.tistory.com/entry/%ED%8C%8C%EC%9D%B4%EC%8D%AC%EC%9C%BC%EB%A1%9C-%EB%94%A5%EB%9F%AC%EB%8B%9D%ED%95%98%EA%B8%B0-CNNConvolution-Neural-Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.CNN은 일상생활 어디에 활용되는가?\n",
    "\n",
    "컨벌루션 뉴럴 네트워크(CNN 또는 ConvNet)는 모델이 직접 이미지, 비디오, 텍스트 또는 사운드를 분류하는 머신 러닝의 한 유형인 딥러닝에 가장 많이 사용되는 알고리즘이다.\n",
    "\n",
    "CNN은 이미지에서 객체, 얼굴, 장면을 인식하기 위한 패턴을 찾는 데 특히 유용하다. CNN은 데이터에서 직접 학습하는데, 패턴을 사용하여 이미지를 분류하고 사람이 수동으로 추출할 필요가 없다.\n",
    "\n",
    "자율 주행 자동차, 얼굴 인식 애플리케이션과 같이 객체 인식과 컴퓨터 비전이 필요한 분야에서 CNN을 많이 사용한다. 응용 분야에 따라 CNN을 처음부터 만들 수도 있고, 데이터셋으로 사전 학습된 모델을 사용할 수도 있다.\n",
    "\n",
    "출처 : https://kr.mathworks.com/solutions/deep-learning/convolutional-neural-network.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.CNN을 활용가능한 알고리즘이나 패키지에는 무엇이 있는가?\n",
    "\n",
    "대표적으로 keras가 있다.\n",
    "\n",
    "Conv2D(convolution)은 각 이미지에서 특정 특징을 활성화하는 컨벌루션 필터 집합에 입력 이미지를 통과시킨다.\n",
    "\n",
    "ReLU(Rectified Linear Unit)는 음수 값을 0에 매핑하고 양수 값을 유지하여 더 빠르고 효과적인 학습을 가능하게 한다. 이때 활성화된 특징만 다음 계층으로 전달되기 때문에 이 과정을 활성화라 부르기도 한다.\n",
    "\n",
    "polling은 비선형 다운샘플링을 수행하고 네트워크에서 학습해야 하는 매개 변수 수를 줄여서 출력을 간소화한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-66e27799eb44>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "img_rows = 28\n",
    "img_cols = 28\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 12  #여러번 학습하면 좋겠지만 시간관계상 3번만 학습하고 결과를 확인합니다.\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(5, 5), strides=(1, 1), padding='same', activation='relu', input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model.add(Conv2D(64, (2, 2), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "hist = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "predicted_result = model.predict(x_test)\n",
    "predicted_labels = np.argmax(predicted_result, axis=1)\n",
    "\n",
    "test_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "count = 0\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "for n in range(16):\n",
    "    count += 1\n",
    "    plt.subplot(4, 4, count)\n",
    "    plt.imshow(x_test[n].reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
    "    tmp = \"Label:\" + str(test_labels[n]) + \", Prediction:\" + str(predicted_labels[n])\n",
    "    plt.title(tmp)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 출처 : https://sdc-james.gitbook.io/onebook/4.-and/5.4.-tensorflow/5.4.2.-cnn-convolutional-neural-network# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.CNN의 핵심 아이디어는 무엇인가? \n",
    "\n",
    "CNN의 가장 큰 특징은 filter를 사람이 수동으로 만드는 것이 아닌, data를 보고 컴퓨터가 학습해서 최종 task에 걸맞도록 직접 만든다는 점이다.\n",
    "\n",
    "input image을 filter(=kernel)와 converlution해서 feature를 추출한다. 이렇게 해서 하나의 feature map을 형성한다. 이렇게 만들어진 feature map은 input image보다 크기가 작아지게 된다. (input image : m * m, feature mpa1 : n * n, m > n)즉, feature map은 전단계에서 feature들만 추출해낸 것이므로, pixel을 비교할 일이 줄어들어 이미지 비교 연산을 할 횟수가 줄어들어 속도적으로도 이점을 얻게 된다. 물론 정보의 손실도 있겠지만, CNN은 특징(패턴)을 뽑아내는 것에 주 목적이 있으므로 큰 영향을 받지 않는다. 이런 과정을 반복하면서 fature map hierarchy를 형성할 수 있다.\n",
    "![CNN_convolution](./images/CNN_convolution.png)\n",
    "convolution\n",
    "\n",
    "![CNN_hierarchy_of_feature_maps](./images/CNN_hierarchy_of_feature_maps.png)\n",
    "hierarchy of feature maps\n",
    "\n",
    "stride : feature map에서 filter(보통 n * n형태)가 이동하는 거리\n",
    "\n",
    "padding : feature filter를 씌우려고 하나, 주위가 존재하지 않아 씌울 수 없을 때, padding이라는 가상의 값을 채워넣어 진행할 수 있게 한다. 보통 값을 0으로 놓는다.\n",
    "![CNN_padding](./images/CNN_padding.png)\n",
    "\n",
    "activation map : 아무리 복잡한 신경망이라도 (w1 * x1 + w2 * x2 + w3 * x3 + w4 * x4 + ... wk + xk)형태를 벗어나진 않는다. 즉, 결국 y = ax + b꼴(선형관계)를 n차원으로 확장한 것이다. 그러나 real world는 비선형적이다. 따라서 비선형 함수(activation function)을 씌워 비선형성을 부여한다. activation map이란 feature map에 activation function을 씌워 비선형성을 부여한 map이다.\n",
    "\n",
    "pooling : features에서 그중에서도 중요한 features를 추출해내는 것이다. 즉, subsampling할 때 어떤 것을 기준으로 할 것인지를 정하는 것이라고 생각하면 된다. 최댓값을 뽑는다면 max pool, 최솟값을 뽑는다면 min pool, 평균값으로 뽑는다면 average poll이런 식이다.\n",
    "![CNN_polling](./images/CNN_polling.png)\n",
    "\n",
    "channel : image를 표현할 수 있는 것의 속성 각각이다. 예를 들어 색적인 측면에서는 RGB로 나타낼 수 있고, 흑백에서는 BW로 나타낼 수 있다. 그런데, original image에서 뽑아낸 feature들은 모두 주요한 특징들 이므로, 이것들도 역시 channel이라고 볼 수 있다. 따라서 feature의 수는 곧 channel의 수라고 할 수 있다. 따라서 대부분의 패키지들에서는 filter size, input channels만 기입하도록 되어있다. input과 feature를 converlution한 것들은 총 channel의 수만큼 나온다. 그것들을 모두 더해서 output으로 만든다. \n",
    "![CNN_input_output_channel](./images/CNN_input_output_channel.png)\n",
    "\n",
    "참고 : https://talkingaboutme.tistory.com/entry/DL-Convolution%EC%9D%98-%EC%A0%95%EC%9D%98"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
